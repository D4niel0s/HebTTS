{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import julius\n",
    "import librosa\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import torchaudio\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "\n",
    "import whisper\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "[ivrit.ai](https://huggingface.co/datasets/ivrit-ai/crowd-transcribe-v5) </br>\n",
    "[HebDB](https://huggingface.co/datasets/SLPRL-HUJI/HebDB) </br>\n",
    "\n",
    "Might use [Hebrew speech Kan](https://huggingface.co/datasets/imvladikon/hebrew_speech_kan) and [Shaul's dataset](https://openslr.org/134/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"SLPRL-HUJI/HebDB\", \"DK_raw\", streaming=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train = data['train'].with_format(\"torch\")\n",
    "# torch_test = data['test'].with_format(\"torch\")\n",
    "\n",
    "for i in torch_train:\n",
    "    res = i\n",
    "    break\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample to 16KHz\n",
    "They did this using julius, librosa runs 200x faster and sounds the fucking same </br>\n",
    "Julius also sucks!! This is straight shit!! I do not know why are there so many methods, and which one to use. </br></br>\n",
    "Librosa is more relatable like \"yo bro you wanna resample? I gotchu!!! use my <b>RESAMPLE</b> method\". </br>\n",
    "Julius is a dickhead it's like \"Oh yeah you should analyze which specific function from within my 40000 modules you want\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = res['audio']['array']\n",
    "sr = res['audio']['sampling_rate'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef = time.time()\n",
    "\n",
    "res_julius = julius.resample._downsample2(audio, 16000)\n",
    "\n",
    "aft = time.time()\n",
    "\n",
    "print('Resampling took ' + str(aft-bef) + ' seconds')\n",
    "IPython.display.Audio(res_julius, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef = time.time()\n",
    "\n",
    "res_librosa = librosa.resample(audio.detach().numpy(), orig_sr=sr, target_sr=16000)\n",
    "\n",
    "aft = time.time()\n",
    "\n",
    "print('Resampling took ' + str(aft-bef) + ' seconds')\n",
    "IPython.display.Audio(res_librosa, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform VAD and split data to short segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_model = load_silero_vad()\n",
    "\n",
    "speech_timestamps = get_speech_timestamps(\n",
    "  res_librosa,\n",
    "  vad_model,\n",
    "  return_seconds=True,\n",
    ")\n",
    "\n",
    "speech_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DURATION = 1.5  # seconds\n",
    "SILENCE_GAP = 0.1   # 100 ms\n",
    "PAD = 0.03          # 30 ms\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "valid_segments = []\n",
    "for seg in speech_timestamps:\n",
    "    duration = seg['end'] - seg['start']\n",
    "\n",
    "    if duration >= MIN_DURATION:\n",
    "        valid_segments.append(seg)\n",
    "\n",
    "# You also may want to merge segments that are closer than SILENCE_GAP\n",
    "merged_segments = []\n",
    "for seg in valid_segments:\n",
    "    if not merged_segments:\n",
    "        merged_segments.append(seg)\n",
    "    else:\n",
    "        prev = merged_segments[-1]\n",
    "        # If current segment starts within the SILENCE_GAP from the previous segmentâ€™s end, merge them\n",
    "        if seg['start'] - prev['end'] < SILENCE_GAP:\n",
    "            merged_segments[-1]['end'] = seg['end']\n",
    "        else:\n",
    "            merged_segments.append(seg)\n",
    "\n",
    "\n",
    "merged_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(merged_segments)=}')\n",
    "lens = np.array([i['end']-i['start'] for i in merged_segments]).mean()\n",
    "print(f'Mean length is {lens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_ACTIVITY_FOR_ENROLL = 3\n",
    "\n",
    "pad_samples = int(PAD * sr)  # Number of samples for 30ms\n",
    "silence = torch.zeros(pad_samples)  # Shape: (1, pad_samples) for mono\n",
    "\n",
    "output_dir = \"DK_samples_1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "whisper_model = whisper.load_model(\"large-v2\")\n",
    "vad_model = load_silero_vad()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "whisper_model.to(device)\n",
    "\n",
    "for i, seg in enumerate(merged_segments):\n",
    "    start_sample = int(seg['start'] * sr)\n",
    "    end_sample   = int(seg['end'] * sr)\n",
    "\n",
    "    segment_waveform = torch.cat((silence, torch.tensor(res_librosa[start_sample:end_sample]), silence))\n",
    "\n",
    "    previous_4_sec = res_librosa[start_sample-(sr*4) : start_sample]\n",
    "\n",
    "    prev_activity = get_speech_timestamps(\n",
    "        previous_4_sec,\n",
    "        vad_model,\n",
    "        return_seconds=True\n",
    "        )\n",
    "    speech_time_in_prev_4_sec = np.array([j['end']-j['start'] for j in prev_activity]).sum()\n",
    "\n",
    "    # We want this segment only if we have an enrollment for it\n",
    "    if speech_time_in_prev_4_sec >= MIN_ACTIVITY_FOR_ENROLL:\n",
    "\n",
    "        while segment_waveform.ndim > 1:\n",
    "            segment_waveform = segment_waveform.squeeze(0)\n",
    "\n",
    "        result = whisper_model.transcribe(segment_waveform, language=\"he\")  # specify Hebrew if needed\n",
    "\n",
    "        with open('data.txt', 'a') as output_data:\n",
    "            output_data.write(output_dir+f'/target_{i}.wav'+'\\t<SEP>\\t'+result['text']+'\\t<SEP>\\t'+output_dir+f'/enrollment_{i}.wav')\n",
    "\n",
    "\n",
    "        # Save target audio to synthesize, and the enrollment\n",
    "        if segment_waveform.ndim == 1:\n",
    "            segment_waveform = segment_waveform.unsqueeze(0)\n",
    "        torchaudio.save(output_dir+f'/target_{i}.wav', segment_waveform, sr)\n",
    "\n",
    "        enroll = torch.cat((silence, torch.tensor(previous_4_sec), silence))\n",
    "        if enroll.ndim == 1:\n",
    "            enroll = enroll.unsqueeze(0)\n",
    "\n",
    "        torchaudio.save(output_dir+f'/enrollment_{i}.wav', enroll, sr)\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe each segment using Whisper V2 large\n",
    "This is a large model (1.5B params) but we can run it now uni servers. We only need some inference. </br>\n",
    "On my CPU (12th gen intel i5-12400) inference took $\\approx45$ sec per audio segment ($\\approx12$ sec audio), so maybe we have hope of finishing this shit quickly on Uni GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"large-v2\")  # loads the Whisper Large V2 model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_dir = \"segmented_wav\"\n",
    "\n",
    "results = []\n",
    "j=0\n",
    "\n",
    "for filename in os.listdir(segmented_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(segmented_dir, filename)\n",
    "\n",
    "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "\n",
    "        result = model.transcribe(audio, language=\"he\")  # specify Hebrew if needed\n",
    "\n",
    "        results.append({filename : result['text']})\n",
    "\n",
    "        j+=1\n",
    "        print(f'Sample {j} fin')\n",
    "        if j == 20: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    print(res, end = 3*'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
