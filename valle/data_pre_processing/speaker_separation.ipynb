{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "import librosa\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "This diarization thing applies more to podcast episodes, or data of this type. Thus [HebDB](https://huggingface.co/datasets/SLPRL-HUJI/HebDB) suits this more. </br>\n",
    "If we can get a raw version of [ivrit.ai](https://huggingface.co/datasets/ivrit-ai/crowd-transcribe-v5) it could also work good. [Shaul's dataset](https://openslr.org/134/) is single speaker and thus doesn't require this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"SLPRL-HUJI/HebDB\", \"GK_raw\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['train'].with_format('torch'):\n",
    "    res = i\n",
    "    break\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = librosa.resample(res['audio']['array'].detach().numpy(), orig_sr=res['audio']['sampling_rate'].item(), target_sr=16000)\n",
    "IPython.display.Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker separation\n",
    "This step will take a long audio and split it up - to separate files where each file is a single speaker audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "waveform = audio[sr*60*23: sr*60*23 + sr*15]\n",
    "IPython.display.Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = torch.tensor(waveform)\n",
    "waveform = waveform.unsqueeze(0)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speech-separation-ami-1.0\",\n",
    "  token=\"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\")\n",
    "\n",
    "\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "diarization, sources = pipeline({\"waveform\": waveform, \"sample_rate\": sr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sources.data.shape[1]):\n",
    "    s = torch.tensor(sources.data[:,i]).unsqueeze(0)\n",
    "    torchaudio.save(f'SPEAKER_{i}.mp3', s, sample_rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, use second notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
